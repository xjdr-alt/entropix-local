{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xc9zeRCJlBSB"
      },
      "source": [
        "# Entropix-local - Research Preview - Llama 3.2 1B/3B"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/xjdr-alt/entropix-local/blob/research/entropix_local_torch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PwmL6ee9jvnp"
      },
      "source": [
        "# Select model size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wvqhYjzhjrGb"
      },
      "outputs": [],
      "source": [
        "SELECTED_MODEL_SIZE = \"1B\" # Selections: 1B, 3B\n",
        "TOKEN = '' # Your HuggingFace token for gated model access"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nyKkWyN76CpV"
      },
      "source": [
        "# Entropix Config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WRwmVGCH6BfO"
      },
      "outputs": [],
      "source": [
        "# Dynamic module config for Entropix Sampler\n",
        "\n",
        "class EntropixConfig:\n",
        "    def __init__(self):\n",
        "        # Sampler state toggles\n",
        "        ## Low Entropy, Low Varentropy: \"flowing with unspoken intent\"\n",
        "        self.state_flowing = False\n",
        "        ## High Entropy, Low Varentropy: \"treading carefully, asking clarifying questions\"\n",
        "        self.state_treading = False\n",
        "        ## Low Entropy, High Varentropy: \"exploring forks in the path\"\n",
        "        self.state_exploring = False\n",
        "        ## High Entropy, High Varentropy: \"resampling in the mist\"\n",
        "        self.state_resampling = False\n",
        "\n",
        "        # Extra sampler state toggles for advanced testing\n",
        "        self.state_extras_agreement = False\n",
        "        self.state_extras_interaction_strength = False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LG1ZrQ7WPysg"
      },
      "source": [
        "# Installs and initial imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YfNdtczYLK8u"
      },
      "outputs": [],
      "source": [
        "!pip install tiktoken\n",
        "!pip install blobfile"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e2IdvdlSltd9"
      },
      "source": [
        "# Config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RHxP8Bd1lvo8"
      },
      "outputs": [],
      "source": [
        "from typing import NamedTuple, Dict, Union, Optional, List\n",
        "from pathlib import Path\n",
        "from functools import partial\n",
        "from dataclasses import dataclass\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "@dataclass\n",
        "class ModelConfig:\n",
        "    \"\"\"Configuration class for model parameters.\"\"\"\n",
        "    dim: int\n",
        "    n_layers: int\n",
        "    n_heads: int\n",
        "    n_kv_heads: int\n",
        "    vocab_size: int\n",
        "    norm_eps: float\n",
        "    rope_theta: float\n",
        "    use_scaled_rope: bool\n",
        "    max_seq_len: int\n",
        "    model_size: str\n",
        "\n",
        "# Define configurations for different models and model sizes\n",
        "MODEL_CONFIGS = {\n",
        "    \"3B\": ModelConfig(\n",
        "        dim=3072,\n",
        "        n_layers=28,\n",
        "        n_heads=24,\n",
        "        n_kv_heads=8,\n",
        "        vocab_size=128256,\n",
        "        norm_eps=1e-05,\n",
        "        rope_theta=500000.0,\n",
        "        use_scaled_rope=True,\n",
        "        max_seq_len=8192,\n",
        "        model_size=\"3B\"\n",
        "    ),\n",
        "    \"1B\": ModelConfig(\n",
        "        dim=2048,\n",
        "        n_layers=16,\n",
        "        n_heads=32,\n",
        "        n_kv_heads=8,\n",
        "        vocab_size=128256,\n",
        "        norm_eps=1e-05,\n",
        "        rope_theta=500000.0,\n",
        "        use_scaled_rope=True,\n",
        "        max_seq_len=4096,\n",
        "        model_size=\"1B\"\n",
        "    )\n",
        "}\n",
        "\n",
        "MODEL_IDS = {\n",
        "    \"3B\": \"meta-llama/Llama-3.2-3B-Instruct\",\n",
        "    \"1B\": \"meta-llama/Llama-3.2-1B-Instruct\"\n",
        "}\n",
        "\n",
        "class ModelParams(NamedTuple):\n",
        "    n_layers: int\n",
        "    n_local_heads: int\n",
        "    n_local_kv_heads: int\n",
        "    head_dim: int\n",
        "    max_seq_len: int\n",
        "    rope_theta: float\n",
        "    use_scaled_rope: bool\n",
        "\n",
        "def get_model_params(config: ModelConfig) -> ModelParams:\n",
        "    \"\"\"Create ModelParams from config.\"\"\"\n",
        "    return ModelParams(\n",
        "        n_layers=config.n_layers,\n",
        "        n_local_heads=config.n_heads,\n",
        "        n_local_kv_heads=config.n_kv_heads,\n",
        "        head_dim=config.dim // config.n_heads,\n",
        "        max_seq_len=config.max_seq_len,\n",
        "        rope_theta=config.rope_theta,\n",
        "        use_scaled_rope=config.use_scaled_rope\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F0fWAb83ghC1"
      },
      "source": [
        "# Download Weights"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NHcDsZzshQji"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "flDeKnQlhS1J"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import ml_dtypes\n",
        "import jax.numpy as jnp\n",
        "import jax\n",
        "from pathlib import Path\n",
        "\n",
        "from transformers import AutoModelForCausalLM\n",
        "from unittest.mock import patch\n",
        "from transformers.dynamic_module_utils import get_imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u3lTK6HWhbFV"
      },
      "outputs": [],
      "source": [
        "def translate_key(in_key: str):\n",
        "    out_key = in_key.replace('.weight', '')\n",
        "    if out_key.startswith('model.'):\n",
        "        out_key = out_key.replace('model.', '')\n",
        "        if out_key.endswith('input_layernorm'):\n",
        "            out_key = out_key.replace('input_layernorm', 'attention_norm')\n",
        "        elif out_key.endswith('mlp.down_proj'):\n",
        "            out_key = out_key.replace('mlp.down_proj', 'feed_forward.w2')\n",
        "        elif out_key.endswith('mlp.gate_proj'):\n",
        "            out_key = out_key.replace('mlp.gate_proj', 'feed_forward.w1')\n",
        "        elif out_key.endswith('mlp.up_proj'):\n",
        "            out_key = out_key.replace('mlp.up_proj', 'feed_forward.w3')\n",
        "        elif out_key.endswith('post_attention_layernorm'):\n",
        "            out_key = out_key.replace('post_attention_layernorm', 'ffn_norm')\n",
        "        elif out_key.endswith('self_attn.k_proj'):\n",
        "            out_key = out_key.replace('self_attn.k_proj', 'attention.wk')\n",
        "        elif out_key.endswith('self_attn.o_proj'):\n",
        "            out_key = out_key.replace('self_attn.o_proj', 'attention.wo')\n",
        "        elif out_key.endswith('self_attn.q_proj'):\n",
        "            out_key = out_key.replace('self_attn.q_proj', 'attention.wq')\n",
        "        elif out_key.endswith('self_attn.v_proj'):\n",
        "            out_key = out_key.replace('self_attn.v_proj', 'attention.wv')\n",
        "        elif out_key.endswith('down_proj'):\n",
        "            out_key = out_key.replace('down_proj', 'w2')\n",
        "        elif out_key.endswith('gate_proj'):\n",
        "            out_key = out_key.replace('gate_proj', 'w1')\n",
        "        elif out_key.endswith('up_proj'):\n",
        "            out_key = out_key.replace('up_proj', 'w3')\n",
        "        elif out_key == 'embed_tokens':\n",
        "            out_key = 'tok_embeddings'\n",
        "        elif out_key == 'norm':\n",
        "            out_key = 'norm'\n",
        "        else:\n",
        "            print(f\"Don't know how to handle {in_key=}\")\n",
        "    elif out_key == 'lm_head':\n",
        "        out_key = 'output'\n",
        "    else:\n",
        "        print(f\"Don't know how to handle {in_key=}\")\n",
        "    return f'{out_key}.weight'\n",
        "\n",
        "\n",
        "def reverse_permute(tensor: torch.Tensor, config: ModelConfig, is_kv: bool = False) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Reverse permute operation with adaptive dimensions based on model size and weight type.\n",
        "\n",
        "    Args:\n",
        "        tensor: Input tensor to be permuted\n",
        "        config: ModelConfig object containing model parameters\n",
        "        is_kv: Boolean flag indicating if the tensor is for key/value weights\n",
        "\n",
        "    Returns:\n",
        "        torch.Tensor: Permuted tensor\n",
        "    \"\"\"\n",
        "    # Determine the number of heads and dimensions based on whether it's a KV weight\n",
        "    if is_kv:\n",
        "        n_heads = config.n_kv_heads\n",
        "        dim1 = (config.dim // config.n_heads) * config.n_kv_heads\n",
        "    else:\n",
        "        n_heads = config.n_heads\n",
        "        dim1 = config.dim\n",
        "\n",
        "    dim2 = config.dim\n",
        "\n",
        "    # Perform the reverse permutation\n",
        "    return tensor.view(n_heads, 2, dim1 // n_heads // 2, dim2).transpose(1, 2).reshape(dim1, dim2)\n",
        "\n",
        "def fixed_get_imports(filename: str | os.PathLike) -> list[str]:\n",
        "    \"\"\"Work around for https://huggingface.co/microsoft/phi-1_5/discussions/72.\"\"\"\n",
        "    if not str(filename).endswith(\"/modeling_deepseek.py\"):\n",
        "        return get_imports(filename)\n",
        "    imports = get_imports(filename)\n",
        "    imports.remove(\"flash_attn\")\n",
        "    return imports\n",
        "\n",
        "def download_weights(model_size: str = \"1B\", out_dir: Optional[Path] = None):\n",
        "    \"\"\"\n",
        "    Download and process weights for specified model size.\n",
        "\n",
        "    Args:\n",
        "        model_size: One of \"1B\" or \"3B\"\n",
        "        out_dir: Optional output directory. If None, uses f'weights/{model_size}-Instruct'\n",
        "    \"\"\"\n",
        "    if model_size not in MODEL_CONFIGS:\n",
        "        raise ValueError(f\"Invalid model size. Choose from: {list(MODEL_CONFIGS.keys())}\")\n",
        "\n",
        "    config = MODEL_CONFIGS[model_size]\n",
        "    model_id = MODEL_IDS[model_size]\n",
        "\n",
        "    if out_dir is None:\n",
        "        out_dir = Path(f'weights/{model_size}-Instruct')\n",
        "\n",
        "    device = torch.device(\"cpu\")\n",
        "    out_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    with patch(\"transformers.dynamic_module_utils.get_imports\", fixed_get_imports):\n",
        "        hf_model = AutoModelForCausalLM.from_pretrained(\n",
        "            model_id,\n",
        "            torch_dtype=torch.bfloat16,\n",
        "            offload_folder=\"/tmp/offload\",\n",
        "            token=TOKEN,\n",
        "            device_map='cpu'\n",
        "        )\n",
        "\n",
        "        with torch.no_grad():\n",
        "            state_dict = hf_model.state_dict()\n",
        "            for hf_name, param in state_dict.items():\n",
        "                print(f' {hf_name}: {param.shape=}')\n",
        "                name = translate_key(hf_name)\n",
        "                param = param.cpu()\n",
        "\n",
        "                # Apply reverse permute for attention weights\n",
        "                if name.endswith('wq.weight'):\n",
        "                    param = reverse_permute(param, config, is_kv=False)\n",
        "                elif name.endswith('wk.weight'):\n",
        "                    param = reverse_permute(param, config, is_kv=True)\n",
        "\n",
        "                # Convert to bfloat16 and save\n",
        "                bf16_np_out = param.cpu().view(dtype=torch.uint16).numpy().view(ml_dtypes.bfloat16)\n",
        "                bf16_out = jnp.asarray(bf16_np_out, dtype=jnp.bfloat16).reshape(*param.shape)\n",
        "                print(f'Writing {hf_name} as {name} to {out_dir}/{name}.npy')\n",
        "                jnp.save(f'{out_dir}/{name}.npy', bf16_out)\n",
        "\n",
        "    # Cleanup\n",
        "    del hf_model\n",
        "    del state_dict\n",
        "    jax.clear_caches()\n",
        "    torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mx72XH3OiYTB"
      },
      "source": [
        "# Load Weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2WdLNnGTicBG"
      },
      "outputs": [],
      "source": [
        "from typing import List, NamedTuple\n",
        "import torch\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import numpy as np\n",
        "\n",
        "import ml_dtypes\n",
        "\n",
        "from pathlib import Path\n",
        "\n",
        "if torch.backends.mps.is_available():\n",
        "    device = torch.device(\"mps\")\n",
        "elif torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "\n",
        "class LayerWeights(NamedTuple):\n",
        "    wq: torch.Tensor\n",
        "    wk: torch.Tensor\n",
        "    wv: torch.Tensor\n",
        "    wo: torch.Tensor\n",
        "    w1: torch.Tensor\n",
        "    w2: torch.Tensor\n",
        "    w3: torch.Tensor\n",
        "    ffn_norm: torch.Tensor\n",
        "    attention_norm: torch.Tensor\n",
        "\n",
        "class XfmrWeights(NamedTuple):\n",
        "    tok_embeddings: torch.Tensor\n",
        "    norm: torch.Tensor\n",
        "    output: torch.Tensor\n",
        "    layer_weights: List[LayerWeights]\n",
        "\n",
        "def load_weights(model_size: str = \"1B\", ckpt_dir: Optional[Path] = None) -> Dict:\n",
        "    \"\"\"\n",
        "    Load weights for specified model size.\n",
        "\n",
        "    Args:\n",
        "        model_size: One of \"1B\", or \"3B\"\n",
        "        ckpt_dir: Optional checkpoint directory. If None, uses f'weights/{model_size}-Instruct'\n",
        "\n",
        "    Returns:\n",
        "        Dict: Model weights\n",
        "    \"\"\"\n",
        "    if model_size not in MODEL_CONFIGS:\n",
        "        raise ValueError(f\"Invalid model size. Choose from: {list(MODEL_CONFIGS.keys())}\")\n",
        "\n",
        "    if ckpt_dir is None:\n",
        "        ckpt_dir = Path(f'weights/{model_size}-Instruct')\n",
        "\n",
        "    config = MODEL_CONFIGS[model_size]\n",
        "    n_layers = config.n_layers\n",
        "    w = {}\n",
        "    layer_weights = []\n",
        "    with torch.inference_mode():\n",
        "      for file in ckpt_dir.glob(\"*.npy\"):\n",
        "        name = '.'.join(str(file).split('/')[-1].split('.')[:-1])\n",
        "        jax_weight = jnp.load(file=file, mmap_mode='r', allow_pickle=True)\n",
        "        #print(f'JAX output (first 30): {jax_weight.flatten()[:30]}')\n",
        "        np_weight = np.array(jax_weight).astype(np.float32)\n",
        "        weight = torch.from_numpy(np_weight).to(torch.bfloat16).to(device)\n",
        "        w[name] = weight.to(device)\n",
        "      for i in range(n_layers):\n",
        "        layer_weights.append(LayerWeights(\n",
        "          wq=w[f'layers.{i}.attention.wq.weight'],\n",
        "          wk=w[f'layers.{i}.attention.wk.weight'],\n",
        "          wv=w[f'layers.{i}.attention.wv.weight'],\n",
        "          wo=w[f'layers.{i}.attention.wo.weight'],\n",
        "          w1=w[f'layers.{i}.feed_forward.w1.weight'],\n",
        "          w2=w[f'layers.{i}.feed_forward.w2.weight'],\n",
        "          w3=w[f'layers.{i}.feed_forward.w3.weight'],\n",
        "          ffn_norm=w[f'layers.{i}.ffn_norm.weight'],\n",
        "          attention_norm=w[f'layers.{i}.attention_norm.weight'],\n",
        "        ))\n",
        "\n",
        "      xfmr_weights = XfmrWeights(\n",
        "        tok_embeddings=w['tok_embeddings.weight'],\n",
        "        norm=w['norm.weight'],\n",
        "        output=w['output.weight'],\n",
        "        layer_weights=layer_weights\n",
        "      )\n",
        "\n",
        "      return xfmr_weights\n",
        "\n",
        "#xfmr_weights = load_weights()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "muWfAoylOo0y"
      },
      "source": [
        "# Tokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z1DB1Bv-2sti"
      },
      "source": [
        "Made quite a few changes to work with tokenizer json instead of tiktoken."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GkIygwTt3Wio"
      },
      "outputs": [],
      "source": [
        "!wget --header=\"Authorization: Bearer {TOKEN}\" \"https://huggingface.co/meta-llama/Llama-3.2-1B/resolve/main/original/tokenizer.model?download=true\" -O tokenizer.model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uG484QmzOqsp"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from logging import getLogger\n",
        "from pathlib import Path\n",
        "from typing import (\n",
        "  AbstractSet,\n",
        "  cast,\n",
        "  Collection,\n",
        "  Dict,\n",
        "  Iterator,\n",
        "  List,\n",
        "  Literal,\n",
        "  Optional,\n",
        "  Sequence,\n",
        "  Union,\n",
        ")\n",
        "\n",
        "import tiktoken\n",
        "\n",
        "from tiktoken.load import load_tiktoken_bpe\n",
        "\n",
        "logger = getLogger(__name__)\n",
        "\n",
        "\n",
        "# The tiktoken tokenizer can handle <=400k chars without\n",
        "# pyo3_runtime.PanicException.\n",
        "TIKTOKEN_MAX_ENCODE_CHARS = 400_000\n",
        "\n",
        "# https://github.com/openai/tiktoken/issues/195\n",
        "# Here we iterate over subsequences and split if we exceed the limit\n",
        "# of max consecutive non-whitespace or whitespace characters.\n",
        "MAX_NO_WHITESPACES_CHARS = 25_000\n",
        "\n",
        "\n",
        "class Tokenizer:\n",
        "  \"\"\"\n",
        "  Tokenizing and encoding/decoding text using the Tiktoken tokenizer.\n",
        "  \"\"\"\n",
        "\n",
        "  special_tokens: Dict[str, int]\n",
        "\n",
        "  num_reserved_special_tokens = 256\n",
        "\n",
        "  pat_str = r\"(?i:'s|'t|'re|'ve|'m|'ll|'d)|[^\\r\\n\\p{L}\\p{N}]?\\p{L}+|\\p{N}{1,3}| ?[^\\s\\p{L}\\p{N}]+[\\r\\n]*|\\s*[\\r\\n]+|\\s+(?!\\S)|\\s+\"  # noqa: E501\n",
        "\n",
        "  def __init__(self, model_path: str):\n",
        "    \"\"\"\n",
        "    Initializes the Tokenizer with a Tiktoken model.\n",
        "\n",
        "    Args:\n",
        "        model_path (str): The path to the Tiktoken model file.\n",
        "    \"\"\"\n",
        "    assert os.path.isfile(model_path), model_path\n",
        "\n",
        "    mergeable_ranks = load_tiktoken_bpe(model_path)\n",
        "    num_base_tokens = len(mergeable_ranks)\n",
        "    special_tokens = [\n",
        "      '<|begin_of_text|>',\n",
        "      '<|end_of_text|>',\n",
        "      '<|reserved_special_token_0|>',\n",
        "      '<|reserved_special_token_1|>',\n",
        "      '<|finetune_right_pad_id|>',\n",
        "      '<|step_id|>',\n",
        "      '<|start_header_id|>',\n",
        "      '<|end_header_id|>',\n",
        "      '<|eom_id|>',  # end of message\n",
        "      '<|eot_id|>',  # end of turn\n",
        "      '<|python_tag|>',\n",
        "    ]\n",
        "    reserved_tokens = [\n",
        "      f'<|reserved_special_token_{2 + i}|>'\n",
        "      for i in range(self.num_reserved_special_tokens - len(special_tokens))\n",
        "    ]\n",
        "    special_tokens = special_tokens + reserved_tokens\n",
        "\n",
        "    self.special_tokens = {token: num_base_tokens + i for i, token in enumerate(special_tokens)}\n",
        "    self.model = tiktoken.Encoding(\n",
        "      name=Path(model_path).name,\n",
        "      pat_str=self.pat_str,\n",
        "      mergeable_ranks=mergeable_ranks,\n",
        "      special_tokens=self.special_tokens,\n",
        "    )\n",
        "\n",
        "    self.n_words: int = num_base_tokens + len(special_tokens)\n",
        "    # BOS / EOS token IDs\n",
        "    self.bos_id: int = self.special_tokens['<|begin_of_text|>']\n",
        "    self.eos_id: int = self.special_tokens['<|end_of_text|>']\n",
        "    self.eot_id: int = self.special_tokens['<|eot_id|>']\n",
        "    self.eom_id: int = self.special_tokens['<|eom_id|>']\n",
        "    self.python_tag_id = self.special_tokens['<|python_tag|>']\n",
        "    self.pad_id: int = self.special_tokens['<|finetune_right_pad_id|>']\n",
        "    self.stop_tokens = [\n",
        "      self.special_tokens['<|eom_id|>'],\n",
        "      self.special_tokens['<|eot_id|>'],\n",
        "    ]\n",
        "\n",
        "  def encode(\n",
        "    self,\n",
        "    s: str,\n",
        "    *,\n",
        "    bos: bool,\n",
        "    eos: bool,\n",
        "    allowed_special: Optional[Union[Literal['all'], AbstractSet[str]]] = None,\n",
        "    disallowed_special: Union[Literal['all'], Collection[str]] = (),\n",
        "  ) -> List[int]:\n",
        "    \"\"\"\n",
        "    Encodes a string into a list of token IDs.\n",
        "\n",
        "    Args:\n",
        "        s (str): The input string to be encoded.\n",
        "        bos (bool): Whether to prepend the beginning-of-sequence token.\n",
        "        eos (bool): Whether to append the end-of-sequence token.\n",
        "        allowed_tokens (\"all\"|set[str]): allowed special tokens in string\n",
        "        disallowed_tokens (\"all\"|set[str]): special tokens that raise an error when in string\n",
        "\n",
        "    Returns:\n",
        "        list[int]: A list of token IDs.\n",
        "\n",
        "    By default, setting disallowed_special=() encodes a string by ignoring\n",
        "    special tokens. Specifically:\n",
        "    - Setting `disallowed_special` to () will cause all text corresponding\n",
        "      to special tokens to be encoded as natural text (insteading of raising\n",
        "      an error).\n",
        "    - Setting `allowed_special` to \"all\" will treat all text corresponding\n",
        "      to special tokens to be encoded as special tokens.\n",
        "    \"\"\"\n",
        "    if allowed_special is None:\n",
        "      allowed_special = set()\n",
        "    assert isinstance(s, str)\n",
        "\n",
        "    substrs = (\n",
        "      substr\n",
        "      for i in range(0, len(s), TIKTOKEN_MAX_ENCODE_CHARS)\n",
        "      for substr in self._split_whitespaces_or_nonwhitespaces(\n",
        "        s[i : i + TIKTOKEN_MAX_ENCODE_CHARS], MAX_NO_WHITESPACES_CHARS\n",
        "      )\n",
        "    )\n",
        "    t: List[int] = []\n",
        "    for substr in substrs:\n",
        "      t.extend(\n",
        "        self.model.encode(\n",
        "          substr,\n",
        "          allowed_special=allowed_special,\n",
        "          disallowed_special=disallowed_special,\n",
        "        )\n",
        "      )\n",
        "    if bos:\n",
        "      t.insert(0, self.bos_id)\n",
        "    if eos:\n",
        "      t.append(self.eos_id)\n",
        "    return t\n",
        "\n",
        "  def decode(self, t: Sequence[int]) -> str:\n",
        "    \"\"\"\n",
        "    Decodes a list of token IDs into a string.\n",
        "\n",
        "    Args:\n",
        "        t (List[int]): The list of token IDs to be decoded.\n",
        "\n",
        "    Returns:\n",
        "        str: The decoded string.\n",
        "    \"\"\"\n",
        "    # Typecast is safe here. Tiktoken doesn't do anything list-related with the sequence.\n",
        "    return self.model.decode(cast(List[int], t))\n",
        "\n",
        "  @staticmethod\n",
        "  def _split_whitespaces_or_nonwhitespaces(s: str, max_consecutive_slice_len: int) -> Iterator[str]:\n",
        "    \"\"\"\n",
        "    Splits the string `s` so that each substring contains no more than `max_consecutive_slice_len`\n",
        "    consecutive whitespaces or consecutive non-whitespaces.\n",
        "    \"\"\"\n",
        "    current_slice_len = 0\n",
        "    current_slice_is_space = s[0].isspace() if len(s) > 0 else False\n",
        "    slice_start = 0\n",
        "\n",
        "    for i in range(len(s)):\n",
        "      is_now_space = s[i].isspace()\n",
        "\n",
        "      if current_slice_is_space ^ is_now_space:\n",
        "        current_slice_len = 1\n",
        "        current_slice_is_space = is_now_space\n",
        "      else:\n",
        "        current_slice_len += 1\n",
        "        if current_slice_len > max_consecutive_slice_len:\n",
        "          yield s[slice_start:i]\n",
        "          slice_start = i\n",
        "          current_slice_len = 1\n",
        "    yield s[slice_start:]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WuT59jyTlNOj"
      },
      "source": [
        "# KVCache"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FoE8AuSDlPtr"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Device selection, tree is like first apple silicion, then cuda, fallback is cpu.\n",
        "if torch.backends.mps.is_available():\n",
        "    device = torch.device(\"mps\")\n",
        "elif torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "\n",
        "class KVCache(nn.Module):\n",
        "    def __init__(self, layers: int, bsz: int, max_seq_len: int, kv_heads: int, head_dim: int):\n",
        "        super(KVCache, self).__init__()\n",
        "        # Initialize k and v as buffers to ensure they're part of the module state\n",
        "        self.register_buffer(\n",
        "            'k',\n",
        "            torch.zeros(\n",
        "                (layers, bsz, max_seq_len, kv_heads, head_dim),\n",
        "                dtype=torch.bfloat16,\n",
        "                device=device\n",
        "            )\n",
        "        )\n",
        "        self.register_buffer(\n",
        "            'v',\n",
        "            torch.zeros(\n",
        "                (layers, bsz, max_seq_len, kv_heads, head_dim),\n",
        "                dtype=torch.bfloat16,\n",
        "                device=device\n",
        "            )\n",
        "        )\n",
        "\n",
        "    @classmethod\n",
        "    def new(cls, layers: int, bsz: int, max_seq_len: int, kv_heads: int, head_dim: int) -> 'KVCache':\n",
        "        \"\"\"Creates a new KVCache instance with initialized k and v tensors.\"\"\"\n",
        "        return cls(layers, bsz, max_seq_len, kv_heads, head_dim)\n",
        "\n",
        "    def update(\n",
        "        self,\n",
        "        xk: torch.Tensor,\n",
        "        xv: torch.Tensor,\n",
        "        layer_idx: int,\n",
        "        cur_pos: int,\n",
        "        n_rep: int\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Updates the cache with new key and value tensors.\n",
        "\n",
        "        Args:\n",
        "            xk (torch.Tensor): New key tensor to insert. Shape should align with (bsz, insert_len, kv_heads, head_dim).\n",
        "            xv (torch.Tensor): New value tensor to insert. Shape should align with (bsz, insert_len, kv_heads, head_dim).\n",
        "            layer_idx (int): The index of the layer to update.\n",
        "            cur_pos (int): The current position in the sequence to start inserting.\n",
        "            n_rep (int): The number of times to repeat the keys and values along the sequence dimension.\n",
        "\n",
        "        Returns:\n",
        "            Tuple[torch.Tensor, torch.Tensor]:\n",
        "                - keys: Updated or repeated keys tensor.\n",
        "                - values: Updated or repeated values tensor.\n",
        "        \"\"\"\n",
        "        # Ensure xk and xv have the correct device and dtype\n",
        "        xk = xk.to(self.k.dtype)\n",
        "        xv = xv.to(self.v.dtype)\n",
        "\n",
        "        # Update the k and v tensors in the specified layer and position\n",
        "        insert_len = xk.size(1)  # Assuming xk shape is (bsz, insert_len, kv_heads, head_dim)\n",
        "        self.k[layer_idx, :, cur_pos:cur_pos+insert_len, :, :] = xk\n",
        "        self.v[layer_idx, :, cur_pos:cur_pos+insert_len, :, :] = xv\n",
        "\n",
        "        if cur_pos == 0:\n",
        "            # If inserting at the beginning, repeat the new keys and values\n",
        "            keys = xk.repeat_interleave(n_rep, dim=2)\n",
        "            values = xv.repeat_interleave(n_rep, dim=2)\n",
        "        else:\n",
        "            # Otherwise, repeat the existing keys and values from the cache\n",
        "            keys = self.k[layer_idx].repeat_interleave(n_rep, dim=2)\n",
        "            values = self.v[layer_idx].repeat_interleave(n_rep, dim=2)\n",
        "\n",
        "        return keys, values, self\n",
        "\n",
        "    def clear(self):\n",
        "        \"\"\"Resets the k and v caches to zeros.\"\"\"\n",
        "        self.k.zero_()\n",
        "        self.v.zero_()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eC0q4zYfMwzR"
      },
      "source": [
        "# Attention Stats"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y-x9GMsYMsy6"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "# Device selection, tree is like first apple silicion, then cuda, fallback is cpu.\n",
        "if torch.backends.mps.is_available():\n",
        "    device = torch.device(\"mps\")\n",
        "elif torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "\n",
        "from typing import NamedTuple\n",
        "\n",
        "class AttnStats(NamedTuple):\n",
        "    entropy: torch.Tensor  # (bsz, n_layers, num_heads)\n",
        "    varentropy: torch.Tensor  # (bsz, n_layers, num_heads)\n",
        "    n_layers: int\n",
        "    n_heads: int\n",
        "\n",
        "    @classmethod\n",
        "    def new(cls, bsz: int, n_layers: int, n_heads: int) -> 'AttnStats':\n",
        "        return cls(\n",
        "            entropy=torch.zeros((bsz, n_layers, n_heads), dtype=torch.float32, device=device),\n",
        "            varentropy=torch.zeros((bsz, n_layers, n_heads), dtype=torch.float32, device=device),\n",
        "            n_layers=n_layers,\n",
        "            n_heads=n_heads\n",
        "        )\n",
        "\n",
        "    @property\n",
        "    def avg_entropy(self):\n",
        "        return self.entropy.sum(dim=-1, keepdim=False)  # Average across heads\n",
        "\n",
        "    @property\n",
        "    def std_error(self):\n",
        "        return torch.sqrt(torch.mean(self.varentropy)) / (self.n_heads * self.n_layers)\n",
        "\n",
        "    def update(self, scores: torch.Tensor, layer_idx: int):\n",
        "        # scores shape: (bsz, n_heads, seqlen, n_words)\n",
        "        probs = torch.nn.functional.softmax(scores, dim=-1)\n",
        "        new_entropy = -torch.sum(torch.where(probs > 0, probs * torch.log(probs), torch.tensor(0.0)), dim=-1)\n",
        "        new_varentropy = torch.sum(probs * (torch.log(probs) + new_entropy.unsqueeze(-1))**2, dim=-1)\n",
        "\n",
        "        # Update entropy and varentropy tensors\n",
        "        self.entropy[:, layer_idx, :] = new_entropy\n",
        "        self.varentropy[:, layer_idx, :] = new_varentropy\n",
        "\n",
        "        return self"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IYjjOvqElY1R"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oQe0q_Jzlap2"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "DEFAULT_MASK_VALUE = -0.7 * float(torch.finfo(torch.float32).max)\n",
        "\n",
        "# Device selection, tree is like first apple silicion, then cuda, fallback is cpu.\n",
        "if torch.backends.mps.is_available():\n",
        "    device = torch.device(\"mps\")\n",
        "elif torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "\n",
        "from typing import Tuple, Optional\n",
        "\n",
        "def rms_norm(x: torch.Tensor, w: torch.Tensor, eps: float = 1e-5) -> torch.Tensor:\n",
        "  return w * (x * torch.rsqrt(torch.pow(x, 2).mean(-1, keepdim=True) + eps))\n",
        "\n",
        "def apply_rotary_emb(xq: torch.Tensor, xk: torch.Tensor, freqs_cis: torch.Tensor, dtype: torch.dtype = torch.float32) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "    reshape_xq = xq.float().reshape(*xq.shape[:-1], -1, 2)\n",
        "    reshape_xk = xk.float().reshape(*xk.shape[:-1], -1, 2)\n",
        "    xq_ = torch.complex(reshape_xq[..., 0], reshape_xq[..., 1])\n",
        "    xk_ = torch.complex(reshape_xk[..., 0], reshape_xk[..., 1])\n",
        "    xq_out = xq_ * freqs_cis.unsqueeze(0).unsqueeze(2)\n",
        "    xk_out = xk_ * freqs_cis.unsqueeze(0).unsqueeze(2)\n",
        "    xq_out = torch.stack((xq_out.real, xq_out.imag), dim=-1).reshape(*xq_out.shape[:-1], -1)\n",
        "    xk_out = torch.stack((xk_out.real, xk_out.imag), dim=-1).reshape(*xk_out.shape[:-1], -1)\n",
        "    return xq_out.to(dtype), xk_out.to(dtype)\n",
        "\n",
        "def attention(x: torch.Tensor, layer_weights: LayerWeights, model_params, cur_pos: int, layer_idx: int, freqs_cis: torch.Tensor, kvcache: KVCache, attn_mask: Optional[torch.Tensor] = None) -> Tuple[torch.Tensor, KVCache, torch.Tensor]:\n",
        "    # Check if x is 2D or 3D and adjust accordingly\n",
        "    if x.dim() == 2:\n",
        "        bsz = 1\n",
        "        seq_len, dim = x.shape\n",
        "        x = x.unsqueeze(0)  # Add batch dimension\n",
        "    else:\n",
        "        bsz, seq_len, dim = x.shape\n",
        "\n",
        "    n_rep = model_params.n_local_heads // model_params.n_local_kv_heads\n",
        "    xq = F.linear(x, layer_weights.wq).view(bsz, seq_len, model_params.n_local_heads, model_params.head_dim)\n",
        "    xk = F.linear(x, layer_weights.wk).view(bsz, seq_len, model_params.n_local_kv_heads, model_params.head_dim)\n",
        "    xv = F.linear(x, layer_weights.wv).view(bsz, seq_len, model_params.n_local_kv_heads, model_params.head_dim)\n",
        "    xq, xk = apply_rotary_emb(xq, xk, freqs_cis=freqs_cis, dtype=xq.dtype)\n",
        "    keys, values, kvcache = kvcache.update(xk, xv, layer_idx, cur_pos, n_rep)\n",
        "    xq = xq.permute(0, 2, 1, 3)  # (bs, n_heads, seqlen, head_dim)\n",
        "    keys = keys.permute(0, 2, 3, 1)  # (bs, n_heads, head_dim, cache_len + seqlen)\n",
        "    values = values.permute(0, 2, 1, 3)  # (bs, n_heads, cache_len + seqlen, head_dim)\n",
        "    scores = torch.matmul(xq, keys)\n",
        "    pre_scores = scores / math.sqrt(model_params.head_dim)\n",
        "    scores = pre_scores.to(torch.float32)  # Always do attention softmax at float32\n",
        "    if cur_pos == 0:\n",
        "        scores = scores + attn_mask\n",
        "    mask = torch.where(scores != 0.0, scores, DEFAULT_MASK_VALUE)\n",
        "    padded_logits = torch.where((mask >= DEFAULT_MASK_VALUE * 0.5), scores, DEFAULT_MASK_VALUE)\n",
        "    scores = F.softmax(padded_logits, dim=-1).to(x.dtype)\n",
        "    output = torch.matmul(scores.to(values.dtype), values)\n",
        "    output = output.transpose(1, 2).contiguous().view(bsz, seq_len, -1)\n",
        "    out = F.linear(output, layer_weights.wo)\n",
        "\n",
        "    # If input was 2D, remove the batch dimension from the output\n",
        "    if x.dim() == 2:\n",
        "        out = out.squeeze(0)\n",
        "\n",
        "    return out, kvcache, pre_scores\n",
        "\n",
        "def feed_forward(x: torch.Tensor, layer_weights: LayerWeights) -> torch.Tensor:\n",
        " return F.linear(F.silu(F.linear(x, layer_weights.w1)) * F.linear(x, layer_weights.w3), layer_weights.w2)\n",
        "\n",
        "def xfmr(xfmr_weights: XfmrWeights, model_params: ModelParams, tokens: torch.Tensor, cur_pos: int, freqs_cis: torch.Tensor, kvcache: KVCache, attn_mask: Optional[torch.Tensor]=None) -> Tuple[torch.Tensor, KVCache, torch.Tensor, AttnStats]:\n",
        "    h = xfmr_weights.tok_embeddings[tokens]\n",
        "    attn_stats = AttnStats.new(\n",
        "        bsz=tokens.shape[0],\n",
        "        n_layers=model_params.n_layers,\n",
        "        n_heads=model_params.n_local_heads\n",
        "    )\n",
        "    for i in range(model_params.n_layers):\n",
        "        norm_x = rms_norm(h, xfmr_weights.layer_weights[i].attention_norm)\n",
        "        h_attn, kvcache, scores = attention(norm_x, xfmr_weights.layer_weights[i], model_params, cur_pos, i, freqs_cis, kvcache, attn_mask=attn_mask)\n",
        "        attn_stats = attn_stats.update(scores[:,:,-1,:], i)\n",
        "        h = h + h_attn\n",
        "        h = h + feed_forward(rms_norm(h, xfmr_weights.layer_weights[i].ffn_norm), xfmr_weights.layer_weights[i])\n",
        "    logits = F.linear(rms_norm(h, xfmr_weights.norm), xfmr_weights.output)\n",
        "    return logits, kvcache, scores, attn_stats"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "myQyMZLiROda"
      },
      "source": [
        "#Sampler Config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y5KoTmHHRQeZ"
      },
      "outputs": [],
      "source": [
        "class SamplerConfig:\n",
        "    def __init__(self, model_size: str = \"1B\"):\n",
        "        \"\"\"\n",
        "        Initialize SamplerConfig with specified model size.\n",
        "\n",
        "        Args:\n",
        "            model_size: One of \"1B\", or \"3B\"\n",
        "        \"\"\"\n",
        "        self.model_size = model_size  # Store model_size as instance variable\n",
        "\n",
        "        if self.model_size == \"1B\":\n",
        "            \"\"\"\n",
        "            Configuration for the sampling strategy, including threshold values for various metrics\n",
        "            and adaptive sampling parameters.\n",
        "            \"\"\"\n",
        "            self.temperature = 0.666\n",
        "            self.top_p = 0.90\n",
        "            self.top_k = 27\n",
        "            self.min_p = 0.03\n",
        "\n",
        "            self.low_logits_entropy_threshold = 0.01\n",
        "            self.medium_logits_entropy_threshold = 0.7\n",
        "            self.high_logits_entropy_threshold = 2.1\n",
        "\n",
        "            self.low_logits_varentropy_threshold = 0.05\n",
        "            self.medium_logits_varentropy_threshold = 2.0\n",
        "            self.high_logits_varentropy_threshold = 5.8\n",
        "\n",
        "            self.low_attention_entropy_threshold = 11.915\n",
        "            self.medium_attention_entropy_threshold = 11.921\n",
        "            self.high_attention_entropy_threshold = 11.926\n",
        "\n",
        "            self.low_attention_varentropy_threshold = 0.001\n",
        "            self.medium_attention_varentropy_threshold = 0.0045\n",
        "            self.high_attention_varentropy_threshold = 0.009\n",
        "\n",
        "            self.low_agreement_threshold = 1.8e-06\n",
        "            self.medium_agreement_threshold = 3.8e-06\n",
        "            self.high_agreement_threshold = 4.8e-06\n",
        "\n",
        "            self.low_interaction_strength_threshold = 0.18\n",
        "            self.medium_interaction_strength_threshold = 0.227\n",
        "            self.high_interaction_strength_threshold = 0.244\n",
        "\n",
        "            self.high_entropy_attention_offset = 1.3\n",
        "            self.high_entropy_attention_coefficient = 0.2\n",
        "\n",
        "            self.low_entropy_interaction_strength_offset = 1.2\n",
        "            self.low_entropy_interaction_strength_coefficient = 0.3\n",
        "\n",
        "            self.high_entropy_varentropy_attention_offset = 2.0\n",
        "            self.high_entropy_varentropy_attention_coefficient = 0.5\n",
        "\n",
        "            self.n_adaptive_samples = 5\n",
        "\n",
        "            self.adaptive_temperature_logits_coefficient = 0.3\n",
        "            self.adaptive_temperature_attention_coefficient = 0.2\n",
        "            self.adaptive_temperature_agreement_coefficient = 0.2\n",
        "            self.adaptive_top_p_coefficient = 0.1\n",
        "            self.adaptive_top_k_interaction_coefficient = 0.3\n",
        "            self.adaptive_top_k_agreement_coefficient = 0.2\n",
        "            self.adaptive_min_p_coefficient = 0.5\n",
        "            self.adaptive_score_logits_entropy_coefficient = 0.1\n",
        "            self.adaptive_score_attention_entropy_coefficient = 0.2\n",
        "            self.adaptive_score_logits_varentropy_coefficient = 0.3\n",
        "            self.adaptive_score_attention_varentropy_coefficient = 0.4\n",
        "            self.adaptive_score_agreement_coefficient = 0.5\n",
        "            self.adaptive_score_interaction_strength_coefficient = 0.6\n",
        "\n",
        "\n",
        "        elif self.model_size == \"3B\":\n",
        "            self.temperature = 0.666\n",
        "            self.top_p = 0.90\n",
        "            self.top_k = 27\n",
        "            self.min_p = 0.03\n",
        "\n",
        "            self.low_logits_entropy_threshold = 0.01\n",
        "            self.medium_logits_entropy_threshold = 0.7\n",
        "            self.high_logits_entropy_threshold = 2.1\n",
        "\n",
        "            self.low_logits_varentropy_threshold = 0.05\n",
        "            self.medium_logits_varentropy_threshold = 2.0\n",
        "            self.high_logits_varentropy_threshold = 5.8\n",
        "\n",
        "            self.low_attention_entropy_threshold = 11.915\n",
        "            self.medium_attention_entropy_threshold = 11.921\n",
        "            self.high_attention_entropy_threshold = 11.926\n",
        "\n",
        "            self.low_attention_varentropy_threshold = 0.001\n",
        "            self.medium_attention_varentropy_threshold = 0.0045\n",
        "            self.high_attention_varentropy_threshold = 0.009\n",
        "\n",
        "            self.low_agreement_threshold = 1.8e-06\n",
        "            self.medium_agreement_threshold = 3.8e-06\n",
        "            self.high_agreement_threshold = 4.8e-06\n",
        "\n",
        "            self.low_interaction_strength_threshold = 0.18\n",
        "            self.medium_interaction_strength_threshold = 0.227\n",
        "            self.high_interaction_strength_threshold = 0.244\n",
        "\n",
        "            self.high_entropy_attention_offset = 1.3\n",
        "            self.high_entropy_attention_coefficient = 0.2\n",
        "\n",
        "            self.low_entropy_interaction_strength_offset = 1.2\n",
        "            self.low_entropy_interaction_strength_coefficient = 0.3\n",
        "\n",
        "            self.high_entropy_varentropy_attention_offset = 2.0\n",
        "            self.high_entropy_varentropy_attention_coefficient = 0.5\n",
        "\n",
        "            self.n_adaptive_samples = 5\n",
        "\n",
        "            self.adaptive_temperature_logits_coefficient = 0.3\n",
        "            self.adaptive_temperature_attention_coefficient = 0.2\n",
        "            self.adaptive_temperature_agreement_coefficient = 0.2\n",
        "            self.adaptive_top_p_coefficient = 0.1\n",
        "            self.adaptive_top_k_interaction_coefficient = 0.3\n",
        "            self.adaptive_top_k_agreement_coefficient = 0.2\n",
        "            self.adaptive_min_p_coefficient = 0.5\n",
        "            self.adaptive_score_logits_entropy_coefficient = 0.1\n",
        "            self.adaptive_score_attention_entropy_coefficient = 0.2\n",
        "            self.adaptive_score_logits_varentropy_coefficient = 0.3\n",
        "            self.adaptive_score_attention_varentropy_coefficient = 0.4\n",
        "            self.adaptive_score_agreement_coefficient = 0.5\n",
        "            self.adaptive_score_interaction_strength_coefficient = 0.6\n",
        "\n",
        "        else:\n",
        "            raise ValueError(f\"Invalid model size: {model_size}. Choose from: 1B, 3B\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AKbwXIUTNR2y"
      },
      "source": [
        "# Sampler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hbIZkVbWNRA6"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from typing import Tuple, Dict\n",
        "from enum import Enum\n",
        "\n",
        "# Device selection\n",
        "if torch.backends.mps.is_available():\n",
        "    device = torch.device(\"mps\")\n",
        "elif torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "\n",
        "LN_2 = 0.69314718056  # ln(2) = 1.0 / LOG2_E\n",
        "\n",
        "class SamplerState(Enum):\n",
        "    FLOWING = \"Flowing with unspoken intent\"\n",
        "    TREADING = \"Treading carefully, asking clarifying questions\"\n",
        "    EXPLORING = \"Exploring forks in the path\"\n",
        "    RESAMPLING = \"Resampling in the mist\"\n",
        "    ADAPTIVE = \"Adaptive Sampling\"\n",
        "\n",
        "def calculate_varentropy_logsoftmax(logits: torch.Tensor, axis: int = -1) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "    \"\"\"Calculate the entropy and varentropy of the probability distribution using logsoftmax.\"\"\"\n",
        "    log_probs = F.log_softmax(logits, dim=axis)\n",
        "    probs = torch.exp(log_probs)\n",
        "    entropy = -torch.sum(probs * log_probs, dim=axis) / LN_2  # Convert to base-2\n",
        "    varentropy = torch.sum(probs * (log_probs / LN_2 + entropy.unsqueeze(-1))**2, dim=axis)\n",
        "    return entropy, varentropy\n",
        "\n",
        "def multinomial_sample_one(probs_sort: torch.Tensor, generator: torch.Generator) -> torch.Tensor:\n",
        "    \"\"\"Samples one token from a multinomial distribution with sorted probabilities.\"\"\"\n",
        "    q = torch.rand(probs_sort.shape, generator=generator, device=probs_sort.device)\n",
        "    return torch.argmax(probs_sort / q, dim=-1, keepdim=True).to(torch.int32)\n",
        "\n",
        "def _sample(logits: torch.Tensor, temperature: float, top_p: float, top_k: int, min_p: float, generator: torch.Generator = None) -> torch.Tensor:\n",
        "    bsz = logits.shape[0]\n",
        "    logit = logits[:, -1]\n",
        "    probs = F.softmax(logit / temperature, dim=-1)\n",
        "\n",
        "    # Apply min_p samplinga\n",
        "    if min_p > 0.0:\n",
        "        p_max = torch.max(probs, dim=-1, keepdim=True).values\n",
        "        indices_to_remove = probs < (min_p * p_max)\n",
        "        logit = torch.where(indices_to_remove, torch.full_like(logit, float('-inf')), logit)\n",
        "        probs = F.softmax(logit, dim=-1)\n",
        "\n",
        "    # Apply top-k sampling\n",
        "    top_k_probs, top_k_indices = torch.topk(probs, k=min(top_k, probs.shape[-1]))\n",
        "    probs_sort = torch.flip(top_k_probs, dims=[-1])\n",
        "    probs_idx = torch.flip(top_k_indices, dims=[-1])\n",
        "    probs_sum = torch.cumsum(probs_sort, dim=-1)\n",
        "    # Apply top-p sampling\n",
        "    mask = torch.where(probs_sum - probs_sort > top_p, torch.tensor(1.0, device=device), torch.tensor(0.0, device=device))\n",
        "    probs_sort = probs_sort * (1 - mask)\n",
        "    probs_sort = probs_sort / torch.sum(probs_sort, dim=-1, keepdim=True)\n",
        "    next_token = multinomial_sample_one(probs_sort, generator)\n",
        "    next_token_g = torch.gather(probs_idx, -1, next_token.reshape(bsz, 1).to(torch.int64))\n",
        "    return next_token_g.to(torch.int32)\n",
        "\n",
        "def calculate_metrics(logits: torch.Tensor, attention_scores: torch.Tensor) -> Dict[str, torch.Tensor]:\n",
        "    entropy, varentropy = calculate_varentropy_logsoftmax(logits)\n",
        "    attention_probs = F.softmax(attention_scores, dim=-1)\n",
        "    attn_entropy = -torch.sum(attention_probs * torch.log2(torch.clamp(attention_probs, 1e-10, 1.0)), dim=-1)\n",
        "    attn_varentropy = torch.var(attn_entropy, dim=1)\n",
        "\n",
        "    attn_varentropy = torch.where(torch.isnan(attn_varentropy), torch.zeros_like(attn_varentropy), attn_varentropy)\n",
        "    mean_attention = torch.mean(attention_probs, dim=1)\n",
        "    agreement = torch.mean(torch.abs(attention_probs - mean_attention.unsqueeze(1)), dim=(1, 2))\n",
        "\n",
        "    interaction_strength = torch.mean(torch.abs(attention_scores), dim=(1, 2, 3))\n",
        "\n",
        "    return {\n",
        "        \"logits_entropy\": torch.mean(entropy),\n",
        "        \"logits_varentropy\": torch.mean(varentropy),\n",
        "        \"attn_entropy\": torch.mean(attn_entropy),\n",
        "        \"attn_varentropy\": torch.mean(attn_varentropy),\n",
        "        \"agreement\": torch.mean(agreement),\n",
        "        \"interaction_strength\": interaction_strength\n",
        "    }\n",
        "\n",
        "def adaptive_sample(logits: torch.Tensor, temperature: float, epsilon: float = 0.01,\n",
        "                   generator: torch.Generator = None) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Perform adaptive sampling by dynamically adjusting the candidate set size based on entropy and varentropy.\n",
        "    \"\"\"\n",
        "    bsz = logits.shape[0]\n",
        "    logit = logits[:, -1]\n",
        "    probs = F.softmax(logit / temperature, dim=-1)\n",
        "\n",
        "    # Sort tokens by probability\n",
        "    sorted_probs, sorted_indices = torch.topk(probs, k=probs.shape[-1], dim=-1)\n",
        "\n",
        "    # Initialize candidate set size\n",
        "    candidate_mask = torch.zeros_like(sorted_probs, dtype=torch.bool, device=logits.device)\n",
        "    cumulative_entropy = torch.zeros(bsz, device=logits.device)\n",
        "    cumulative_varentropy = torch.zeros(bsz, device=logits.device)\n",
        "    # Initial entropy calculation\n",
        "    previous_entropy = -torch.sum(sorted_probs[0] * torch.log2(torch.clamp(sorted_probs[0], 1e-10, 1.0)))\n",
        "\n",
        "    i = 0\n",
        "    while i < sorted_probs.shape[-1]:\n",
        "        current_prob = sorted_probs[:, i]\n",
        "\n",
        "        # Update entropy and varentropy with current token\n",
        "        current_entropy = -torch.sum(current_prob * torch.log2(torch.clamp(current_prob, 1e-10, 1.0)))\n",
        "        current_varentropy = torch.sum(current_prob * (torch.log2(torch.clamp(current_prob, 1e-10, 1.0)) +\n",
        "                                                     cumulative_entropy.unsqueeze(-1))**2)\n",
        "\n",
        "        entropy_reduction = cumulative_entropy - current_entropy\n",
        "        varentropy_reduction = cumulative_varentropy - current_varentropy\n",
        "\n",
        "        # Update mask where entropy reduction is sufficient\n",
        "        candidate_mask[:, i] = entropy_reduction >= epsilon\n",
        "\n",
        "        # Update cumulative values\n",
        "        cumulative_entropy = torch.where(entropy_reduction >= epsilon,\n",
        "                                       cumulative_entropy.clone(),\n",
        "                                       current_entropy)\n",
        "        cumulative_varentropy = torch.where(entropy_reduction >= epsilon,\n",
        "                                          cumulative_varentropy.clone(),\n",
        "                                          current_varentropy)\n",
        "\n",
        "        # Check continuation condition\n",
        "        if not torch.any(entropy_reduction >= epsilon) or i >= sorted_probs.shape[-1] - 1:\n",
        "            break\n",
        "\n",
        "        i += 1\n",
        "\n",
        "    # Mask out tokens not in the candidate set\n",
        "    candidate_probs = sorted_probs * candidate_mask.float()\n",
        "    candidate_probs = candidate_probs / torch.sum(candidate_probs, dim=-1, keepdim=True)\n",
        "\n",
        "    # Sample from the final candidate set\n",
        "    next_token = multinomial_sample_one(candidate_probs, generator)\n",
        "    next_token_g = torch.gather(sorted_indices, -1, next_token.to(torch.int64))\n",
        "\n",
        "    return next_token_g.to(torch.int32)\n",
        "\n",
        "def sample(gen_tokens: torch.Tensor, logits: torch.Tensor, attention_scores: torch.Tensor, cfg: SamplerConfig, entropix_cfg: EntropixConfig,\n",
        "           clarifying_question_token: int = 2026, generator: torch.Generator = torch.Generator(device=device).manual_seed(1337)) -> Tuple[torch.Tensor, SamplerState]:\n",
        "    metrics = calculate_metrics(logits, attention_scores)\n",
        "    ent, vent = metrics[\"logits_entropy\"], metrics[\"logits_varentropy\"]\n",
        "    attn_ent, attn_vent = metrics[\"attn_entropy\"], metrics[\"attn_varentropy\"]\n",
        "    agreement = metrics[\"agreement\"]\n",
        "    interaction_strength = metrics[\"interaction_strength\"]\n",
        "\n",
        "    # Low Entropy, Low Varentropy: \"flowing with unspoken intent\"\n",
        "    if entropix_cfg.state_flowing and (ent < cfg.low_logits_entropy_threshold and\n",
        "        vent < cfg.low_logits_varentropy_threshold and\n",
        "        attn_ent < cfg.low_attention_entropy_threshold and\n",
        "        attn_vent < cfg.low_attention_varentropy_threshold and\n",
        "        (not entropix_cfg.state_extras_agreement or agreement < cfg.low_agreement_threshold) and\n",
        "        (not entropix_cfg.state_extras_interaction_strength or interaction_strength < cfg.low_interaction_strength_threshold)):\n",
        "        sampler_state = SamplerState.FLOWING\n",
        "        sampled_token = torch.argmax(logits[:, -1], dim=-1, keepdim=True).to(torch.int32)\n",
        "        return sampled_token, sampler_state\n",
        "\n",
        "    # High Entropy, Low Varentropy: \"treading carefully, asking clarifying questions\"\n",
        "    elif entropix_cfg.state_treading and (ent > cfg.high_logits_entropy_threshold and\n",
        "          vent < cfg.low_logits_varentropy_threshold and\n",
        "          attn_ent < cfg.low_attention_entropy_threshold and\n",
        "          attn_vent < cfg.low_attention_varentropy_threshold and\n",
        "          (not entropix_cfg.state_extras_agreement or agreement < cfg.low_agreement_threshold) and\n",
        "          (not entropix_cfg.state_extras_interaction_strength or interaction_strength < cfg.low_interaction_strength_threshold)):\n",
        "        sampler_state = SamplerState.TREADING\n",
        "        # Insert a clarifying question token if not already present\n",
        "        if not torch.isin(gen_tokens[:, -1], torch.tensor([clarifying_question_token], device=device)).any():\n",
        "            sampled_token = torch.tensor([[clarifying_question_token]], dtype=torch.int32, device=device)\n",
        "            return sampled_token, sampler_state\n",
        "        else:\n",
        "            # If we've just asked a question, sample with slightly higher temperature\n",
        "            temp_adj = cfg.high_entropy_attention_offset + cfg.high_entropy_attention_coefficient * attn_ent\n",
        "            sampled_token = _sample(\n",
        "                logits,\n",
        "                temperature=min(1.5, cfg.temperature * temp_adj),\n",
        "                top_p=cfg.top_p,\n",
        "                top_k=cfg.top_k,\n",
        "                min_p=cfg.min_p,\n",
        "                generator=generator\n",
        "            )\n",
        "            return sampled_token, sampler_state\n",
        "\n",
        "    # Low Entropy, High Varentropy: \"exploring forks in the path\"\n",
        "    elif entropix_cfg.state_exploring and (ent < cfg.high_logits_entropy_threshold and\n",
        "          vent > cfg.high_logits_varentropy_threshold and\n",
        "          attn_ent < cfg.low_attention_entropy_threshold and\n",
        "          attn_vent > cfg.high_attention_varentropy_threshold and\n",
        "          (not entropix_cfg.state_extras_agreement or agreement < cfg.low_agreement_threshold) and\n",
        "          (not entropix_cfg.state_extras_interaction_strength or interaction_strength > cfg.low_interaction_strength_threshold)):\n",
        "        sampler_state = SamplerState.EXPLORING\n",
        "        temp_adj = cfg.low_entropy_interaction_strength_offset + cfg.low_entropy_interaction_strength_coefficient * interaction_strength\n",
        "        top_k_adj = max(5, int(cfg.top_k * (1 + 0.5 * (1 - agreement))))\n",
        "        sampled_token = _sample(\n",
        "            logits,\n",
        "            temperature=min(1.5, cfg.temperature * temp_adj),\n",
        "            top_p=cfg.top_p,\n",
        "            top_k=top_k_adj,\n",
        "            min_p=cfg.min_p,\n",
        "            generator=generator\n",
        "        )\n",
        "        return sampled_token, sampler_state\n",
        "\n",
        "    # High Entropy, High Varentropy: \"resampling in the mist\"\n",
        "    elif entropix_cfg.state_resampling and (ent > cfg.medium_logits_entropy_threshold and\n",
        "          vent > cfg.high_logits_varentropy_threshold and\n",
        "          attn_ent > cfg.high_attention_entropy_threshold and\n",
        "          attn_vent > cfg.high_attention_varentropy_threshold and\n",
        "          (not entropix_cfg.state_extras_agreement or agreement > cfg.high_agreement_threshold) and\n",
        "          (not entropix_cfg.state_extras_interaction_strength or interaction_strength > cfg.high_interaction_strength_threshold)):\n",
        "        sampler_state = SamplerState.RESAMPLING\n",
        "        # Use high temperature and adjusted top_p based on attention metrics\n",
        "        temp_adj = cfg.high_entropy_varentropy_attention_offset + cfg.high_entropy_varentropy_attention_coefficient * attn_vent\n",
        "        top_p_adj = max(0.5, cfg.top_p - cfg.high_entropy_attention_coefficient * attn_ent)\n",
        "        sampled_token = _sample(\n",
        "            logits,\n",
        "            temperature=max(2.0, cfg.temperature * temp_adj),\n",
        "            top_p=top_p_adj,\n",
        "            top_k=cfg.top_k,\n",
        "            min_p=cfg.min_p,\n",
        "            generator=generator\n",
        "        )\n",
        "        return sampled_token, sampler_state\n",
        "\n",
        "    # All other cases: use adaptive sampling\n",
        "    else:\n",
        "        sampler_state = SamplerState.ADAPTIVE\n",
        "        '''temperature = 0.666\n",
        "        sampled_token = adaptive_sample(\n",
        "            logits,\n",
        "            temperature=temperature,\n",
        "            epsilon=0.1,\n",
        "            generator=generator\n",
        "        )'''\n",
        "        logits_uncertainty = ent + vent\n",
        "        attn_uncertainty = attn_ent + attn_vent\n",
        "\n",
        "        temperature = cfg.temperature * (\n",
        "            1 +\n",
        "            cfg.adaptive_temperature_logits_coefficient * ent +\n",
        "            cfg.adaptive_temperature_attention_coefficient * attn_ent -\n",
        "            cfg.adaptive_temperature_agreement_coefficient * agreement\n",
        "        )\n",
        "        top_p = torch.clamp(\n",
        "            (cfg.top_p * (1 + cfg.adaptive_top_p_coefficient * attn_vent)).clone().detach(),\n",
        "            0.1,\n",
        "            1.0\n",
        "        )\n",
        "        top_k = int(torch.clamp(\n",
        "            torch.round(torch.tensor(cfg.top_k) * (\n",
        "                1 +\n",
        "                cfg.adaptive_top_k_interaction_coefficient * interaction_strength.item() -\n",
        "                cfg.adaptive_top_k_agreement_coefficient * agreement.item()\n",
        "            )),\n",
        "            min=1,\n",
        "            max=100\n",
        "        ).item())\n",
        "        min_p = torch.clamp(\n",
        "            (cfg.min_p * (1 - cfg.adaptive_min_p_coefficient * vent)).clone().detach(),\n",
        "            0.01,\n",
        "            0.5\n",
        "        )\n",
        "\n",
        "        samples = []\n",
        "        for _ in range(cfg.n_adaptive_samples):\n",
        "            sample = _sample(\n",
        "                logits,\n",
        "                temperature=temperature,\n",
        "                top_p=top_p,\n",
        "                top_k=top_k,\n",
        "                min_p=min_p,\n",
        "                generator=generator\n",
        "            )\n",
        "            samples.append(sample)\n",
        "\n",
        "        def score_sample(sample):\n",
        "            # Ensure sample is a 1D tensor of indices\n",
        "            sample_indices = sample.view(-1).to(torch.long)\n",
        "\n",
        "            # Create one-hot encoding\n",
        "            one_hot = F.one_hot(sample_indices, num_classes=logits.shape[-1])\n",
        "\n",
        "            # Calculate log probability\n",
        "            log_probs = F.log_softmax(logits[:, -1], dim=-1)\n",
        "            log_prob = torch.sum(log_probs * one_hot, dim=-1)\n",
        "\n",
        "            confidence_score = (\n",
        "                (1 - ent / cfg.high_logits_entropy_threshold) * cfg.adaptive_score_logits_entropy_coefficient +\n",
        "                (1 - attn_ent / cfg.high_attention_entropy_threshold) * cfg.adaptive_score_attention_entropy_coefficient +\n",
        "                (1 - vent / cfg.high_logits_varentropy_threshold) * cfg.adaptive_score_logits_varentropy_coefficient +\n",
        "                (1 - attn_vent / cfg.high_attention_varentropy_threshold) * cfg.adaptive_score_attention_varentropy_coefficient +\n",
        "                (agreement / cfg.high_agreement_threshold) * cfg.adaptive_score_agreement_coefficient +\n",
        "                (interaction_strength / cfg.high_interaction_strength_threshold) * cfg.adaptive_score_interaction_strength_coefficient\n",
        "            )\n",
        "            return log_prob + confidence_score\n",
        "\n",
        "        sample_scores = torch.stack([score_sample(sample) for sample in samples])\n",
        "        best_sample_idx = torch.argmax(sample_scores)\n",
        "        sampled_token = samples[best_sample_idx]\n",
        "        return sampled_token, sampler_state"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SzaxLJKYmEqt"
      },
      "source": [
        "# Main"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8uKZTergBzPg"
      },
      "outputs": [],
      "source": [
        "from typing import NamedTuple, Optional, Tuple\n",
        "import os\n",
        "\n",
        "import plotly.graph_objects as go\n",
        "import numpy as np\n",
        "import json\n",
        "from datetime import datetime\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import math\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.colors import ListedColormap, BoundaryNorm\n",
        "from matplotlib.patches import Patch\n",
        "\n",
        "from pathlib import Path\n",
        "from functools import partial\n",
        "\n",
        "from IPython.display import display, Markdown\n",
        "\n",
        "# Device selection, tree is like first apple silicion, then cuda, fallback is cpu.\n",
        "if torch.backends.mps.is_available():\n",
        "    device = torch.device(\"mps\")\n",
        "elif torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "\n",
        "print(f\"Using device: {device}\")\n",
        "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n",
        "torch.cuda.empty_cache()\n",
        "torch.set_float32_matmul_precision('high')\n",
        "\n",
        "def apply_scaling(freqs: torch.Tensor) -> torch.Tensor:\n",
        "    SCALE_FACTOR = 8.0\n",
        "    LOW_FREQ_FACTOR = 1.0\n",
        "    HIGH_FREQ_FACTOR = 4.0\n",
        "    OLD_CONTEXT_LEN = 2048  # original llama3 length\n",
        "\n",
        "    low_freq_wavelen = OLD_CONTEXT_LEN / LOW_FREQ_FACTOR\n",
        "    high_freq_wavelen = OLD_CONTEXT_LEN / HIGH_FREQ_FACTOR\n",
        "\n",
        "    def scale_freq(freq: torch.Tensor) -> torch.Tensor:\n",
        "        wavelen = 2 * torch.pi / freq\n",
        "\n",
        "        # Calculate smooth factor\n",
        "        smooth = (OLD_CONTEXT_LEN / wavelen - LOW_FREQ_FACTOR) / (HIGH_FREQ_FACTOR - LOW_FREQ_FACTOR)\n",
        "        smooth = torch.clamp(smooth, 0.0, 1.0)  # Ensure smooth is between 0 and 1\n",
        "\n",
        "        # Calculate scaled frequency\n",
        "        scaled = (1 - smooth) * freq / SCALE_FACTOR + smooth * freq\n",
        "\n",
        "        # Apply conditional scaling\n",
        "        scaled = torch.where(\n",
        "            wavelen < high_freq_wavelen,\n",
        "            freq,  # No scaling\n",
        "            torch.where(\n",
        "                wavelen > low_freq_wavelen,\n",
        "                freq / SCALE_FACTOR,  # Apply scaling factor\n",
        "                scaled  # Apply smooth scaling\n",
        "            )\n",
        "        )\n",
        "        return scaled\n",
        "\n",
        "    scaled_freqs = torch.vmap(scale_freq)(freqs)\n",
        "\n",
        "    return scaled_freqs\n",
        "\n",
        "def precompute_freqs_cis(dim: int, end: int, theta: float = 10000.0, use_scaled: bool = False, dtype: torch.dtype = torch.float32) -> torch.Tensor:\n",
        "    freqs = 1.0 / (theta ** (torch.arange(0, dim, 2, dtype=dtype, device=device)[: (dim // 2)] / dim))\n",
        "    if use_scaled:\n",
        "        freqs = apply_scaling(freqs)\n",
        "\n",
        "    t = torch.arange(end, dtype=dtype, device=device).unsqueeze(1)  # Shape: (end, 1)\n",
        "    freqs = freqs.unsqueeze(0)  # Shape: (1, dim//2)\n",
        "    freqs = t * freqs  # Broadcasting to shape: (end, dim//2)\n",
        "    return torch.exp(1j * freqs)\n",
        "\n",
        "def build_attn_mask(seqlen: int, start_pos: int) -> torch.Tensor:\n",
        "  mask = None\n",
        "  if seqlen > 1:\n",
        "      mask = torch.full((seqlen, seqlen), float(\"-inf\"))\n",
        "      mask = torch.triu(mask, diagonal=1)\n",
        "      mask = torch.hstack([torch.zeros((seqlen, start_pos)), mask]).to(torch.float32).to(device)\n",
        "  return mask\n",
        "\n",
        "class EntropixModel:\n",
        "    def __init__(self, model_size: str = \"1B\"):\n",
        "        \"\"\"\n",
        "        Initialize EntropixModel with specified model size.\n",
        "\n",
        "        Args:\n",
        "            model_size: One of \"1B\", or \"3B\"\n",
        "        \"\"\"\n",
        "        if model_size not in MODEL_CONFIGS:\n",
        "            raise ValueError(f\"Invalid model size. Choose from: {list(MODEL_CONFIGS.keys())}\")\n",
        "\n",
        "        self.model_size = model_size\n",
        "        self.config = MODEL_CONFIGS[model_size]\n",
        "        self.model_params = get_model_params(self.config)\n",
        "        self.xfmr_weights = load_weights(model_size=model_size)\n",
        "        self.tokenizer = Tokenizer('tokenizer.model')\n",
        "        self.sampler_config = SamplerConfig(model_size)\n",
        "        self.entropix_config = EntropixConfig()\n",
        "        self.generator = torch.Generator(device=device).manual_seed(1337)\n",
        "\n",
        "    def visualize_token_entropy_varentropy(self, metrics_data, generated_tokens):\n",
        "        # Extract data\n",
        "        entropies = np.array(metrics_data['logits_entropy'])\n",
        "        varentropies = np.array(metrics_data['logits_varentropy'])\n",
        "        attention_entropies = np.array(metrics_data['attention_entropy'])\n",
        "        attention_varentropies = np.array(metrics_data['attention_varentropy'])\n",
        "\n",
        "        # Ensure all arrays have the same length\n",
        "        min_length = min(len(entropies), len(varentropies), len(attention_entropies), len(attention_varentropies), len(generated_tokens))\n",
        "        entropies = entropies[:min_length]\n",
        "        varentropies = varentropies[:min_length]\n",
        "        attention_entropies = attention_entropies[:min_length]\n",
        "        attention_varentropies = attention_varentropies[:min_length]\n",
        "        generated_tokens = generated_tokens[:min_length]\n",
        "\n",
        "        positions = np.arange(min_length)\n",
        "\n",
        "        # Create hover text\n",
        "        hover_text = [\n",
        "            f\"Token: {self.tokenizer.decode([token]) or 'Unknown'}<br>\"\n",
        "            f\"Position: {i}<br>\"\n",
        "            f\"Logits Entropy: {entropies[i]:.4f}<br>\"\n",
        "            f\"Logits Varentropy: {varentropies[i]:.4f}<br>\"\n",
        "            f\"Attention Entropy: {attention_entropies[i]:.4f}<br>\"\n",
        "            f\"Attention Varentropy: {attention_varentropies[i]:.4f}\"\n",
        "            for i, token in enumerate(generated_tokens)\n",
        "        ]\n",
        "\n",
        "        # Create the 3D scatter plot\n",
        "        fig = go.Figure()\n",
        "\n",
        "        # Add logits entropy/varentropy scatter\n",
        "        fig.add_trace(go.Scatter3d(\n",
        "            x=entropies,\n",
        "            y=varentropies,\n",
        "            z=positions,\n",
        "            mode='markers',\n",
        "            marker=dict(\n",
        "                size=5,\n",
        "                color=entropies,\n",
        "                colorscale='Viridis',\n",
        "                opacity=0.8,\n",
        "                colorbar=dict(title=\"Logits Entropy\", x=0.85),\n",
        "            ),\n",
        "            text=hover_text,\n",
        "            hoverinfo='text',\n",
        "            name='Logits Entropy/Varentropy'\n",
        "        ))\n",
        "\n",
        "        # Add attention entropy/varentropy scatter\n",
        "        fig.add_trace(go.Scatter3d(\n",
        "            x=attention_entropies,\n",
        "            y=attention_varentropies,\n",
        "            z=positions,\n",
        "            mode='markers',\n",
        "            marker=dict(\n",
        "                size=5,\n",
        "                color=attention_entropies,\n",
        "                colorscale='Plasma',\n",
        "                opacity=0.8,\n",
        "                colorbar=dict(title=\"Attention Entropy\", x=1.0),\n",
        "            ),\n",
        "            text=hover_text,\n",
        "            hoverinfo='text',\n",
        "            name='Attention Entropy/Varentropy'\n",
        "        ))\n",
        "\n",
        "        # Calculate the limits for x, y, and z\n",
        "        logits_x_min, logits_x_max = min(entropies), max(entropies)\n",
        "        logits_y_min, logits_y_max = min(varentropies), max(varentropies)\n",
        "        attention_x_min, attention_x_max = min(attention_entropies), max(attention_entropies)\n",
        "        attention_y_min, attention_y_max = min(attention_varentropies), max(attention_varentropies)\n",
        "        z_min, z_max = min(positions), max(positions)\n",
        "\n",
        "        # Function to create threshold planes\n",
        "        def create_threshold_plane(threshold, axis, color, name, data_type):\n",
        "            if data_type == 'logits':\n",
        "                x_min, x_max = logits_x_min, logits_x_max\n",
        "                y_min, y_max = logits_y_min, logits_y_max\n",
        "            else:  # attention\n",
        "                x_min, x_max = attention_x_min, attention_x_max\n",
        "                y_min, y_max = attention_y_min, attention_y_max\n",
        "\n",
        "            if axis == 'x':\n",
        "                return go.Surface(\n",
        "                    x=[[threshold, threshold], [threshold, threshold]],\n",
        "                    y=[[y_min, y_max], [y_min, y_max]],\n",
        "                    z=[[z_min, z_min], [z_max, z_max]],\n",
        "                    colorscale=[[0, color], [1, color]],\n",
        "                    showscale=False,\n",
        "                    name=name,\n",
        "                    visible=False\n",
        "                )\n",
        "            elif axis == 'y':\n",
        "                return go.Surface(\n",
        "                    x=[[x_min, x_max], [x_min, x_max]],\n",
        "                    y=[[threshold, threshold], [threshold, threshold]],\n",
        "                    z=[[z_min, z_min], [z_max, z_max]],\n",
        "                    colorscale=[[0, color], [1, color]],\n",
        "                    showscale=False,\n",
        "                    name=name,\n",
        "                    visible=False\n",
        "                )\n",
        "\n",
        "        # Add threshold planes\n",
        "        thresholds = [\n",
        "            ('logits_entropy', 'x', [\n",
        "                (self.sampler_config.low_logits_entropy_threshold, 'rgba(255, 0, 0, 0.2)'),\n",
        "                (self.sampler_config.medium_logits_entropy_threshold, 'rgba(0, 255, 0, 0.2)'),\n",
        "                (self.sampler_config.high_logits_entropy_threshold, 'rgba(0, 0, 255, 0.2)')\n",
        "            ], 'logits'),\n",
        "            ('logits_varentropy', 'y', [\n",
        "                (self.sampler_config.low_logits_varentropy_threshold, 'rgba(255, 165, 0, 0.2)'),\n",
        "                (self.sampler_config.medium_logits_varentropy_threshold, 'rgba(165, 42, 42, 0.2)'),\n",
        "                (self.sampler_config.high_logits_varentropy_threshold, 'rgba(128, 0, 128, 0.2)')\n",
        "            ], 'logits'),\n",
        "            ('attention_entropy', 'x', [\n",
        "                (self.sampler_config.low_attention_entropy_threshold, 'rgba(255, 192, 203, 0.2)'),\n",
        "                (self.sampler_config.medium_attention_entropy_threshold, 'rgba(0, 255, 255, 0.2)'),\n",
        "                (self.sampler_config.high_attention_entropy_threshold, 'rgba(255, 255, 0, 0.2)')\n",
        "            ], 'attention'),\n",
        "            ('attention_varentropy', 'y', [\n",
        "                (self.sampler_config.low_attention_varentropy_threshold, 'rgba(70, 130, 180, 0.2)'),\n",
        "                (self.sampler_config.medium_attention_varentropy_threshold, 'rgba(244, 164, 96, 0.2)'),\n",
        "                (self.sampler_config.high_attention_varentropy_threshold, 'rgba(50, 205, 50, 0.2)')\n",
        "            ], 'attention')\n",
        "        ]\n",
        "\n",
        "        for threshold_type, axis, threshold_list, data_type in thresholds:\n",
        "            for threshold, color in threshold_list:\n",
        "                fig.add_trace(create_threshold_plane(threshold, axis, color, f'{threshold_type.replace(\"_\", \" \").title()} Threshold: {threshold}', data_type))\n",
        "\n",
        "        # Create buttons for toggling views\n",
        "        buttons = [\n",
        "            dict(\n",
        "                label='Show All',\n",
        "                method='update',\n",
        "                args=[{'visible': [True] * len(fig.data)}]\n",
        "            ),\n",
        "            dict(\n",
        "                label='Hide All',\n",
        "                method='update',\n",
        "                args=[{'visible': [True, True] + [False] * (len(fig.data) - 2)}]\n",
        "            ),\n",
        "            dict(\n",
        "                label='Logits Only',\n",
        "                method='update',\n",
        "                args=[{'visible': [True, False] + [True if i < 6 else False for i in range(len(fig.data) - 2)]}]\n",
        "            ),\n",
        "            dict(\n",
        "                label='Attention Only',\n",
        "                method='update',\n",
        "                args=[{'visible': [False, True] + [True if i >= 6 else False for i in range(len(fig.data) - 2)]}]\n",
        "            )\n",
        "        ]\n",
        "\n",
        "        # Update layout\n",
        "        fig.update_layout(\n",
        "            scene=dict(\n",
        "                xaxis_title='Entropy',\n",
        "                yaxis_title='Varentropy',\n",
        "                zaxis_title='Token Position',\n",
        "                aspectmode='manual',\n",
        "                aspectratio=dict(x=1, y=1, z=0.5),\n",
        "            ),\n",
        "            margin=dict(l=0, r=0, b=0, t=40),\n",
        "            title='',\n",
        "            updatemenus=[dict(\n",
        "                type=\"buttons\",\n",
        "                direction=\"right\",\n",
        "                x=0.0,\n",
        "                y=1.1,\n",
        "                xanchor='left',\n",
        "                yanchor='top',\n",
        "                pad={\"r\": 10, \"t\": 10},\n",
        "                showactive=True,\n",
        "                buttons=buttons\n",
        "            )],\n",
        "            autosize=True,\n",
        "            legend=dict(x=0.02, y=0.98, xanchor='left', yanchor='top'),\n",
        "        )\n",
        "        '''\n",
        "        # Export data to file\n",
        "        export_data = {\n",
        "            \"tokens\": [self.tokenizer.decode([token]) for token in generated_tokens],\n",
        "            \"logits_entropy\": metrics_data['logits_entropy'],\n",
        "            \"logits_varentropy\": metrics_data['logits_varentropy'],\n",
        "            \"attention_entropy\": metrics_data['attention_entropy'],\n",
        "            \"attention_varentropy\": metrics_data['attention_varentropy'],\n",
        "            \"thresholds\": {\n",
        "                \"logits_entropy\": {\n",
        "                    \"low\": self.sampler_config.low_logits_entropy_threshold,\n",
        "                    \"medium\": self.sampler_config.medium_logits_entropy_threshold,\n",
        "                    \"high\": self.sampler_config.high_logits_entropy_threshold\n",
        "                },\n",
        "                \"logits_varentropy\": {\n",
        "                    \"low\": self.sampler_config.low_logits_varentropy_threshold,\n",
        "                    \"medium\": self.sampler_config.medium_logits_varentropy_threshold,\n",
        "                    \"high\": self.sampler_config.high_logits_varentropy_threshold\n",
        "                },\n",
        "                \"attention_entropy\": {\n",
        "                    \"low\": self.sampler_config.low_attention_entropy_threshold,\n",
        "                    \"medium\": self.sampler_config.medium_attention_entropy_threshold,\n",
        "                    \"high\": self.sampler_config.high_attention_entropy_threshold\n",
        "                },\n",
        "                \"attention_varentropy\": {\n",
        "                    \"low\": self.sampler_config.low_attention_varentropy_threshold,\n",
        "                    \"medium\": self.sampler_config.medium_attention_varentropy_threshold,\n",
        "                    \"high\": self.sampler_config.high_attention_varentropy_threshold\n",
        "                }\n",
        "            }\n",
        "        }\n",
        "\n",
        "        # Generate a unique filename with timestamp\n",
        "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "        filename = f\"entropy_data_{timestamp}.json\"\n",
        "\n",
        "        # Save the data to a file\n",
        "        with open(filename, 'w') as f:\n",
        "            json.dump(export_data, f, indent=2)\n",
        "\n",
        "        print(f\"Data exported to {filename}\")\n",
        "        '''\n",
        "        return fig\n",
        "\n",
        "    def generate(self, prompt, max_tokens=1800, debug=True):\n",
        "        # Initialize lists to store metrics\n",
        "        metrics_data = {\n",
        "            'logits_entropy': [],\n",
        "            'logits_varentropy': [],\n",
        "            'attention_entropy': [],\n",
        "            'attention_varentropy': []\n",
        "        }\n",
        "        sampler_states = []\n",
        "        generated_tokens = []\n",
        "\n",
        "        with torch.inference_mode():\n",
        "            tokens = self.tokenizer.encode(\"<|start_header_id|>user<|end_header_id|>\\n\" + prompt + \"<|eot_id|>\\n<|start_header_id|>assistant<|end_header_id|>\\n\", bos=True, eos=False, allowed_special='all')\n",
        "            tokens = torch.tensor([tokens], dtype=torch.long).to(device)\n",
        "            bsz, seqlen = tokens.shape\n",
        "            cur_pos = 0\n",
        "            attn_mask = build_attn_mask(seqlen, cur_pos)\n",
        "            freqs_cis = precompute_freqs_cis(self.model_params.head_dim, self.model_params.max_seq_len, self.model_params.rope_theta, self.model_params.use_scaled_rope)\n",
        "            kvcache = KVCache.new(self.model_params.n_layers, bsz, self.model_params.max_seq_len, self.model_params.n_local_kv_heads, self.model_params.head_dim).to(device)\n",
        "\n",
        "            logits, kvcache, scores, _ = xfmr(self.xfmr_weights, self.model_params, tokens, cur_pos, freqs_cis[:seqlen], kvcache, attn_mask=attn_mask)\n",
        "            next_token, sampler_state = sample(tokens, logits, scores, self.sampler_config, self.entropix_config, generator=self.generator)\n",
        "\n",
        "            metrics = calculate_metrics(logits, scores)\n",
        "            for key in metrics_data.keys():\n",
        "                if key in metrics:\n",
        "                    metrics_data[key].append(metrics[key].item())\n",
        "            sampler_states.append(sampler_state)\n",
        "\n",
        "            gen_tokens = next_token\n",
        "            output = self.tokenizer.decode([next_token.item()])\n",
        "            generated_tokens.append(next_token.item())\n",
        "            cur_pos = seqlen\n",
        "            stop = torch.tensor([128001, 128008, 128009], device=device, dtype=torch.int32).to(device)\n",
        "\n",
        "            while cur_pos < max_tokens:\n",
        "                cur_pos += 1\n",
        "                logits, kvcache, scores, _ = xfmr(self.xfmr_weights, self.model_params, next_token, cur_pos, freqs_cis[cur_pos:cur_pos+1], kvcache)\n",
        "                next_token, sampler_state = sample(gen_tokens, logits, scores, self.sampler_config, self.entropix_config, generator=self.generator)\n",
        "\n",
        "                metrics = calculate_metrics(logits, scores)\n",
        "                for key in metrics_data.keys():\n",
        "                    if key in metrics:\n",
        "                        metrics_data[key].append(metrics[key].item())\n",
        "                sampler_states.append(sampler_state)\n",
        "                metrics_data['attention_entropy'].append(metrics['attn_entropy'].item())\n",
        "                metrics_data['attention_varentropy'].append(metrics['attn_varentropy'].item())\n",
        "                generated_tokens.append(next_token.item())\n",
        "                gen_tokens = torch.cat((gen_tokens, next_token), dim=1)\n",
        "                output += self.tokenizer.decode(next_token.tolist()[0])\n",
        "                if torch.isin(next_token, stop).any():\n",
        "                    break\n",
        "\n",
        "        if debug:\n",
        "            #self.debug_visualize_metrics(metrics_data)\n",
        "            self.visualize_sampler_metrics(metrics_data['logits_entropy'], metrics_data['logits_varentropy'], sampler_states)\n",
        "            fig = self.visualize_token_entropy_varentropy(metrics_data, generated_tokens)\n",
        "            fig.show()\n",
        "        return output\n",
        "\n",
        "    def debug_visualize_metrics(self, metrics_data):\n",
        "        fig, axs = plt.subplots(3, 2, figsize=(15, 15))\n",
        "        fig.suptitle('Debug Visualization of Sampler Metrics', fontsize=16)\n",
        "\n",
        "        for idx, (key, values) in enumerate(metrics_data.items()):\n",
        "            if values:  # Only plot if we have data for this metric\n",
        "                row = idx // 2\n",
        "                col = idx % 2\n",
        "                axs[row, col].plot(values)\n",
        "                axs[row, col].set_title(key)\n",
        "                axs[row, col].set_xlabel('Generation Step')\n",
        "                axs[row, col].set_ylabel('Value')\n",
        "                axs[row, col].grid(True)\n",
        "\n",
        "        # Add entropy_attention visualization if we have both metrics\n",
        "        if metrics_data['logits_entropy'] and metrics_data['attention_entropy']:\n",
        "            axs[2, 0].scatter(metrics_data['logits_entropy'], metrics_data['attention_entropy'])\n",
        "            axs[2, 0].set_title('entropy_attention')\n",
        "            axs[2, 0].set_xlabel('Logits Entropy')\n",
        "            axs[2, 0].set_ylabel('Attention Entropy')\n",
        "            axs[2, 0].grid(True)\n",
        "\n",
        "        # Add entropy_interaction_strength visualization if we have both metrics\n",
        "        if metrics_data['logits_entropy'] and metrics_data['interaction_strength']:\n",
        "            axs[2, 1].scatter(metrics_data['logits_entropy'], metrics_data['interaction_strength'])\n",
        "            axs[2, 1].set_title('entropy_interaction_strength')\n",
        "            axs[2, 1].set_xlabel('Logits Entropy')\n",
        "            axs[2, 1].set_ylabel('Interaction Strength')\n",
        "            axs[2, 1].grid(True)\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    def visualize_sampler_metrics(self, entropies, varentropies, sampler_states):\n",
        "        fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 4), height_ratios=[4, 1], sharex=True)\n",
        "\n",
        "        # Plot entropy and varentropy\n",
        "        x = range(len(entropies))\n",
        "        ax1.plot(x, entropies, label='Entropy', color='blue')\n",
        "        ax1.plot(x, varentropies, label='Varentropy', color='red')\n",
        "        ax1.set_ylabel('Value')\n",
        "        ax1.set_title('Entropy and Varentropy over Generation Steps')\n",
        "        ax1.legend()\n",
        "        ax1.grid(True)\n",
        "\n",
        "        # Define colors in the same order as SamplerState\n",
        "        colors = ['lightblue', 'lightgreen', 'orange', 'pink', 'purple']\n",
        "        cmap = ListedColormap(colors)\n",
        "\n",
        "        # Explicitly map each SamplerState to its corresponding index\n",
        "        state_to_num = {\n",
        "            SamplerState.FLOWING: 0,\n",
        "            SamplerState.TREADING: 1,\n",
        "            SamplerState.EXPLORING: 2,\n",
        "            SamplerState.RESAMPLING: 3,\n",
        "            SamplerState.ADAPTIVE: 4\n",
        "        }\n",
        "\n",
        "        # Map sampler states to numerical values\n",
        "        numeric_states = [state_to_num[state] for state in sampler_states]\n",
        "\n",
        "        # Define normalization to map each integer to a color without interpolation\n",
        "        norm = BoundaryNorm(boundaries=[-0.5 + i for i in range(len(colors)+1)],\n",
        "                          ncolors=cmap.N,\n",
        "                          clip=True)\n",
        "\n",
        "        # Plot color-coded sampler states\n",
        "        im = ax2.imshow([numeric_states], cmap=cmap, norm=norm, aspect='auto',\n",
        "                      extent=[0, len(numeric_states), 0, 1])\n",
        "        ax2.set_yticks([])\n",
        "        ax2.set_title('Sampler State over Generation Steps')\n",
        "\n",
        "        mapped_colors = [colors[state_to_num[state]] for state in sampler_states]\n",
        "\n",
        "        # Create a custom legend for sampler states\n",
        "        legend_elements = [Patch(facecolor=colors[state_to_num[state]], edgecolor='black', label=state.value)\n",
        "                          for state in SamplerState]\n",
        "        ax2.legend(handles=legend_elements, loc='upper center', bbox_to_anchor=(0.5, -0.15),\n",
        "                  ncol=3, fancybox=True, shadow=True)\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "# Function to initialize the model (to be run once)\n",
        "def initialize_model(model_size: str = \"1B\") -> None:\n",
        "    \"\"\"\n",
        "    Initialize global model instance with specified size.\n",
        "\n",
        "    Args:\n",
        "        model_size: One of \"1B\", or \"3B\"\n",
        "    \"\"\"\n",
        "    global entropix_model\n",
        "\n",
        "    if model_size not in MODEL_CONFIGS:\n",
        "        raise ValueError(f\"Invalid model size. Choose from: {list(MODEL_CONFIGS.keys())}\")\n",
        "\n",
        "    print(f\"Initializing {model_size} model...\")\n",
        "    entropix_model = EntropixModel(model_size)\n",
        "    print(f\"{model_size} model initialized and ready to use!\")\n",
        "\n",
        "# Function to generate text (can be used in multiple cells)\n",
        "def generate_text(prompt):\n",
        "    global entropix_model\n",
        "    if 'entropix_model' not in globals():\n",
        "        print(\"Model not initialized. Please run initialize_model() first.\")\n",
        "        return\n",
        "    response = entropix_model.generate(prompt)\n",
        "    # Display the response with proper newline rendering\n",
        "    display(Markdown(response))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bJF-oEDfLjST"
      },
      "source": [
        "# Generate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d9wEhcLokNdo"
      },
      "outputs": [],
      "source": [
        "download_weights(SELECTED_MODEL_SIZE)\n",
        "#load_weights(SELECTED_MODEL_SIZE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yuqUXYDPCCju"
      },
      "outputs": [],
      "source": [
        "torch.cuda.empty_cache()\n",
        "initialize_model(SELECTED_MODEL_SIZE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7bKYM8xtmyB4"
      },
      "outputs": [],
      "source": [
        "generate_text(\"Give quantum qubit state representation theoric description.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FFVcP_j5PUjM"
      },
      "outputs": [],
      "source": [
        "generate_text(\"Tell me a short story about a robot learning to paint.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qqFTtaivB5pZ"
      },
      "outputs": [],
      "source": [
        "generate_text(\"What is the capital city of Spain?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KIO5pwA_CGKa"
      },
      "outputs": [],
      "source": [
        "generate_text(\"Which number is larger, 9.9 or 9.11? Find result step by step\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
